{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEN3450 COMPETITION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, BayesianRidge, LassoLarsIC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "- drop the id column for training data, but remember it for the test data\n",
    "- extract the label (house price) and transform it to log space\n",
    "- join the training and testing data into a single dataframe for easier transformation with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>topic_01</th>\n",
       "      <th>topic_02</th>\n",
       "      <th>topic_03</th>\n",
       "      <th>topic_04</th>\n",
       "      <th>topic_05</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>531</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.404896</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.494651</td>\n",
       "      <td>0.028905</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.429850</td>\n",
       "      <td>0.100705</td>\n",
       "      <td>0.041431</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>370</td>\n",
       "      <td>0.559889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.359459</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022245</td>\n",
       "      <td>0.306718</td>\n",
       "      <td>0.022231</td>\n",
       "      <td>0.022224</td>\n",
       "      <td>0.626582</td>\n",
       "      <td>0.437409</td>\n",
       "      <td>0.071184</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0               9               531         0.503788               1.0   \n",
       "1              10               370         0.559889               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  num_videos  \\\n",
       "0                  0.665635          9               0         1           0   \n",
       "1                  0.698198          2               2         0           0   \n",
       "\n",
       "   average_token_length  num_keywords  data_channel_is_lifestyle  \\\n",
       "0              4.404896             7                          0   \n",
       "1              4.359459             9                          0   \n",
       "\n",
       "   data_channel_is_entertainment  data_channel_is_bus  data_channel_is_socmed  \\\n",
       "0                              1                    0                       0   \n",
       "1                              0                    0                       0   \n",
       "\n",
       "   data_channel_is_tech  data_channel_is_world  kw_min_avg  kw_max_avg  \\\n",
       "0                     0                      0         0.0         0.0   \n",
       "1                     1                      0         0.0         0.0   \n",
       "\n",
       "   kw_avg_avg  self_reference_min_shares  self_reference_max_shares  \\\n",
       "0         0.0                        0.0                          0   \n",
       "1         0.0                     8500.0                       8500   \n",
       "\n",
       "   self_reference_avg_sharess  weekday_is_monday  weekday_is_tuesday  \\\n",
       "0                         0.0                  1                   0   \n",
       "1                      8500.0                  1                   0   \n",
       "\n",
       "   weekday_is_wednesday  weekday_is_thursday  weekday_is_friday  \\\n",
       "0                     0                    0                  0   \n",
       "1                     0                    0                  0   \n",
       "\n",
       "   weekday_is_saturday  weekday_is_sunday  is_weekend  topic_01  topic_02  \\\n",
       "0                    0                  0           0  0.028573  0.419300   \n",
       "1                    0                  0           0  0.022245  0.306718   \n",
       "\n",
       "   topic_03  topic_04  topic_05  global_subjectivity  \\\n",
       "0  0.494651  0.028905  0.028572             0.429850   \n",
       "1  0.022231  0.022224  0.626582             0.437409   \n",
       "\n",
       "   global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0                   0.100705                    0.041431   \n",
       "1                   0.071184                    0.029730   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.020716             0.666667             0.333333   \n",
       "1                    0.027027             0.523810             0.476190   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  \n",
       "0            0.000000                  0.000000  \n",
       "1            0.642857                  0.214286  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data \n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# get the training and testing data\n",
    "x_train_raw = train_data.drop(['shares'],axis=1)\n",
    "x_test_raw = test_data\n",
    "\n",
    "# remember the lenghts so we can split the data later\n",
    "train_length = len(train_data)\n",
    "test_length = len(test_data)\n",
    "\n",
    "# concatenate train + test data into one df\n",
    "all_data = pd.concat([x_train_raw, x_test_raw])\n",
    "\n",
    "# get the training data - house prices\n",
    "# normal prices have a skew so adjust them to log space\n",
    "y_train = train_data['shares']\n",
    "\n",
    "\n",
    "#display the data\n",
    "pd.set_option('display.max_columns', 500)\n",
    "x_test_raw.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming features\n",
    "- missing -> replace with median \n",
    "- categorical -> One hot encoding\n",
    "- numerical -> use Robust scaler that is robust to outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a nice trick to find out numeric vs categorical features\n",
    "numerical_features = x_train_raw.columns[x_train_raw.dtypes != 'object']\n",
    "categorical_features = x_train_raw.columns[x_train_raw.dtypes == 'object']\n",
    "\n",
    "# encode missing numbers as a special large number\n",
    "all_data[numerical_features] = all_data[numerical_features].fillna(0)\n",
    "\n",
    "# encode missing data as a special category -> missing\n",
    "all_data[categorical_features] = all_data[categorical_features].fillna(\"Missing\")\n",
    "\n",
    "all_data.head(2)\n",
    "\n",
    "# transform numeric variables \n",
    "ss = RobustScaler() # StandardScaler()\n",
    "#all_data[numerical_features] = ss.fit_transform(all_data[numerical_features])\n",
    "\n",
    "# transform categorical variables\n",
    "all_data = pd.get_dummies(data=all_data, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-split the data back to original proportionsÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw features vs features in transformed model: 44 vs 44 features\n",
      "Length train vs test: 29733  vs  9911\n"
     ]
    }
   ],
   "source": [
    "# re-split again\n",
    "x_train = all_data.head(train_length)\n",
    "x_test = all_data.tail(test_length)\n",
    "\n",
    "print(\"Raw features vs features in transformed model:\", x_train_raw.shape[1] ,'vs', x_train.shape[1], 'features')\n",
    "print(\"Length train vs test:\" , train_length , ' vs ', test_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01, max_depth=7, \n",
    "                                max_features='sqrt',min_samples_leaf=15, min_samples_split=10, \n",
    "                                loss='ls',random_state = 13)\n",
    "# est.fit(x_train, y_train)\n",
    "Xgbr2 = GradientBoostingRegressor(n_estimators=3000, learning_rate=1, max_depth=18, \n",
    "                                max_features='sqrt',min_samples_leaf=15, min_samples_split=10, \n",
    "                                loss='ls',random_state = 23)\n",
    "\n",
    "# est.fit(x_train, y_train)\n",
    "Xgbr3 = GradientBoostingRegressor(n_estimators=2500, learning_rate=0.1, max_depth=15, \n",
    "                                max_features='sqrt',min_samples_leaf=5, min_samples_split=5, \n",
    "                                loss='ls',random_state = 33)\n",
    "\n",
    "Xgbr4 = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.01, max_depth=10, \n",
    "                                max_features='sqrt',min_samples_leaf=5, min_samples_split=5, \n",
    "                                loss='ls',random_state = 33)\n",
    "\n",
    "Xgbr5 = GradientBoostingRegressor(n_estimators=200, learning_rate=0.001, max_depth=3, \n",
    "                                max_features='sqrt',min_samples_leaf=5, min_samples_split=35, \n",
    "                                loss='ls',random_state = 44)\n",
    "\n",
    "Xgbr6 = GradientBoostingRegressor(n_estimators=500, learning_rate=0.001, max_depth=3, \n",
    "                                max_features='sqrt',min_samples_leaf=5, min_samples_split=25, \n",
    "                                loss='ls',random_state = 44)\n",
    "\n",
    "Xgbr7 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.001, max_depth=10, \n",
    "                                max_features='sqrt',min_samples_leaf=5, min_samples_split=55, \n",
    "                                loss='ls',random_state = 44)\n",
    "\n",
    "Xgbr8 = GradientBoostingRegressor(n_estimators=50, learning_rate=0.001, max_depth=20, \n",
    "                                max_features='sqrt',min_samples_leaf=5, min_samples_split=35, \n",
    "                                loss='ls',random_state = 44)\n",
    "\n",
    "Xgbr9 = GradientBoostingRegressor(n_estimators=10, learning_rate=0.001, max_depth=15, \n",
    "                                max_features='sqrt',min_samples_leaf=5, min_samples_split=20, \n",
    "                                loss='ls',random_state = 44)\n",
    "\n",
    "Xgbr10 = GradientBoostingRegressor(n_estimators=100, learning_rate=0.001, max_depth=5, \n",
    "                                max_features='sqrt',min_samples_leaf=5, min_samples_split=10, \n",
    "                                loss='ls',random_state = 44)\n",
    "\n",
    "Xgbr11 = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01, max_depth=7, \n",
    "                                max_features='auto',min_samples_leaf=15, min_samples_split=10, \n",
    "                                loss='ls',random_state = 13)\n",
    "Xgbr12 = GradientBoostingRegressor(n_estimators=3000, learning_rate=1, max_depth=18, \n",
    "                                max_features='log2',min_samples_leaf=15, min_samples_split=10, \n",
    "                                loss='ls',random_state = 23)\n",
    "\n",
    "Xgbr13 = GradientBoostingRegressor(n_estimators=3500, learning_rate=0.5, max_depth=15, \n",
    "                                max_features='log2',min_samples_leaf=5, min_samples_split=5, \n",
    "                                loss='ls',random_state = 33)\n",
    "\n",
    "Xgbr14 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, max_depth=10, \n",
    "                                max_features='auto',min_samples_leaf=5, min_samples_split=5, \n",
    "                                loss='ls',random_state = 99)\n",
    "\n",
    "Xgbr15 = GradientBoostingRegressor(n_estimators=20, learning_rate=0.01, max_depth=3, \n",
    "                                max_features='log2',min_samples_leaf=5, min_samples_split=35, \n",
    "                                loss='ls',random_state = 44)\n",
    "\n",
    "Xgbr16 = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=3, \n",
    "                                max_features='auto',min_samples_leaf=5, min_samples_split=25, \n",
    "                                loss='ls',random_state = 44)\n",
    "\n",
    "Xgbr17 = GradientBoostingRegressor(n_estimators=100, learning_rate=0.05, max_depth=10, \n",
    "                                max_features='log2',min_samples_leaf=5, min_samples_split=55, \n",
    "                                loss='ls',random_state = 88)\n",
    "\n",
    "Xgbr18 = GradientBoostingRegressor(n_estimators=250, learning_rate=0.5, max_depth=20, \n",
    "                                max_features='log2',min_samples_leaf=5, min_samples_split=35, \n",
    "                                loss='ls',random_state = 77)\n",
    "\n",
    "Xgbr19 = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=15, \n",
    "                                max_features='log2',min_samples_leaf=5, min_samples_split=20, \n",
    "                                loss='ls',random_state = 66)\n",
    "\n",
    "Xgbr20 = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=5, \n",
    "                                max_features='log2',min_samples_leaf=5, min_samples_split=10, \n",
    "                                loss='ls',random_state = 55)\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression(normalize=False)\n",
    "regr2 = linear_model.LinearRegression(normalize=True)\n",
    "regr3 = linear_model.LinearRegression(normalize=True)\n",
    "\n",
    "lasso =  Lasso(alpha =0.0005)\n",
    "lasso2 =  Lasso(alpha =0.005)\n",
    "lasso3 =  Lasso(alpha =0.05) \n",
    "lasso4 =  Lasso(alpha =0.5) \n",
    "lasso5 =  Lasso(alpha = 0.9) \n",
    "\n",
    "ridge = Ridge(alpha =0.0005)\n",
    "ridge2 = Ridge(alpha =0.005)\n",
    "ridge3 = Ridge(alpha =0.05)\n",
    "ridge4 = Ridge(alpha =0.5)\n",
    "ridge5 = Ridge(alpha =0.9)\n",
    "\n",
    "ENet1 =  ElasticNet(alpha=0.0005)\n",
    "ENet2 =  ElasticNet(alpha=0.005)\n",
    "ENet3 =  ElasticNet(alpha=0.05)\n",
    "ENet4 =  ElasticNet(alpha=0.5)\n",
    "ENet5 =  ElasticNet(alpha=0.9)\n",
    "\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   \n",
    "\n",
    "averaged_models = AveragingModels(models = (Xgbr,Xgbr2,Xgbr3,Xgbr4,Xgbr5,Xgbr6,Xgbr7,Xgbr8,Xgbr9,Xgbr10,\n",
    "                                            Xgbr11,Xgbr12,Xgbr13,Xgbr14,Xgbr15,Xgbr16,Xgbr17,Xgbr18,Xgbr19,Xgbr20,\n",
    "                                            regr,regr2,regr3,lasso,lasso2,lasso3,lasso4,lasso5,\n",
    "                                            ridge,ridge2,ridge3,ridge4,ridge5,ENet1,ENet2,ENet3,ENet4,ENet5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/SD/Applications/miniconda3/lib/python3.6/site-packages/scipy/linalg/basic.py:1018: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n",
      "/Volumes/SD/Applications/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "best_model = averaged_models.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = best_model.predict(x_test)\n",
    "result = pd.DataFrame({'shares': y_test.ravel()})\n",
    "result['id'] = result.index + 1\n",
    "\n",
    "result = result[['id','shares']]\n",
    "result[result <0] = y_train.mean() / 4\n",
    "result.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, shares]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['shares'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
